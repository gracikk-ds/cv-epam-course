{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85afc8e7-42f8-4732-b49c-f5857e623aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 13:07:49.834150: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-02-22 13:07:49.834192: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/opt/conda/lib/python3.7/site-packages/nncf/torch/__init__.py:26: UserWarning: NNCF provides best results with torch==1.9.1, while current torch version is 1.9.0 - consider switching to torch==1.9.1\n",
      "  curr=torch.__version__\n",
      "/opt/conda/lib/python3.7/site-packages/nncf/torch/dynamic_graph/patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n",
      "/opt/conda/lib/python3.7/site-packages/nncf/torch/dynamic_graph/patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import logging\n",
    "import functools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import (\n",
    "    Tuple, \n",
    "    Optional,\n",
    "    Callable\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import albumentations as albu\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.callbacks import QuantizationAwareTraining\n",
    "\n",
    "import torch\n",
    "\n",
    "from nncf import NNCFConfig  # Important - should be imported directly after torch\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from openvino.inference_engine import IECore\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "import tensorboard\n",
    "import torch.nn as nn\n",
    "from torchmetrics import IoU\n",
    "from torch.nn.utils import prune\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a734b-5204-42e2-b2a9-d8b25cc4f992",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcac336-3b30-41a7-a9ec-6f4e212c4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "\n",
    "class Transforms:\n",
    "    def __init__(self, segment=\"train\"):\n",
    "        if segment == \"train\":\n",
    "            transforms = [\n",
    "                albu.LongestMaxSize(max_size=230, always_apply=True, p=1),\n",
    "                albu.OneOf(\n",
    "                    [\n",
    "                        albu.ColorJitter(hue=0.01, saturation=0.01),\n",
    "                        albu.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05)\n",
    "                    ], p=0.3\n",
    "                ),\n",
    "                albu.ShiftScaleRotate(border_mode=1, rotate_limit=30, p=0.3),\n",
    "                albu.PadIfNeeded(\n",
    "                    min_height=230,\n",
    "                    min_width=230,\n",
    "                    always_apply=True,\n",
    "                    border_mode=0,\n",
    "                    value=(255, 255, 255),\n",
    "                ),\n",
    "                albu.OneOf(\n",
    "                    [\n",
    "                        albu.RandomCrop(width=224, height=224),\n",
    "                        albu.Resize(width=224, height=224)\n",
    "                    ], p=1\n",
    "                ),\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "            ]\n",
    "        else:\n",
    "            transforms = [\n",
    "                albu.LongestMaxSize(max_size=224, always_apply=True, p=1),\n",
    "                albu.PadIfNeeded(\n",
    "                    min_height=224,\n",
    "                    min_width=224,\n",
    "                    always_apply=True,\n",
    "                    border_mode=0,\n",
    "                    value=(255, 255, 255),\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        self.transforms = albu.Compose(transforms)\n",
    "\n",
    "    def __call__(self, img, msk, *args, **kwargs):\n",
    "        return self.transforms(image=np.array(img), mask=np.array(msk))\n",
    "\n",
    "\n",
    "class SegmentationDataSet(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: list,\n",
    "        masks: list,\n",
    "        transform: Optional[Callable] = None,\n",
    "        preprocessing: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine path lists\n",
    "        self.images = sorted(images)\n",
    "        self.masks = sorted(masks)\n",
    "\n",
    "        # transformation\n",
    "        self.transform = transform\n",
    "\n",
    "        # preprocessing\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        # getting len info\n",
    "        self.len_ = len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # read data\n",
    "        image = Image.open(str(self.images[index]))\n",
    "        image = np.array(image)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            if image.shape[2] == 4:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        mask = Image.open(str(self.masks[index])).convert('L')\n",
    "        mask = np.array(mask)\n",
    "        # print(str(self.images[index]), f\": shape {image.shape}, {mask.shape} \\n\")\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.transform is not None:\n",
    "            try:\n",
    "                sample = self.transform(img=image, msk=mask)\n",
    "                image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "            except:\n",
    "                print(str(self.images[index]))\n",
    "                print(image.shape)\n",
    "                print(mask.shape)\n",
    "                raise AssertionError()\n",
    "           \n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            try:\n",
    "                mask = mask[..., np.newaxis]\n",
    "                sample = self.preprocessing(image=image, mask=mask)\n",
    "                image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "            except Exception:\n",
    "                print(str(self.images[index]))\n",
    "                print(image.shape)\n",
    "                print(str(self.masks[index]))\n",
    "                print(mask.shape)\n",
    "\n",
    "        return image.float(), mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2d9580-9f4d-4208-8dc3-926f0aa3406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def BarcodeSegmentation(\n",
    "    classes=None,\n",
    "    model=\"FPN\",\n",
    "    encoder_name: str = \"efficientnet-b3\",\n",
    "    encoder_weights: str = \"imagenet\",\n",
    "    activation: str = \"sigmoid\",\n",
    "    decoder_attention_type: str = \"scse\",\n",
    "    model_weights_path=None,\n",
    "):\n",
    "    if classes is None:\n",
    "        classes = [\"barcode\"]\n",
    "    if hasattr(smp, model):\n",
    "        model_class = getattr(smp, model)\n",
    "        if isinstance(model_class, type):\n",
    "            kwargs = {}\n",
    "            if model == \"UnetPlusPlus\":\n",
    "                kwargs = {\"decoder_attention_type\": decoder_attention_type}\n",
    "            model = model_class(\n",
    "                encoder_name=encoder_name,\n",
    "                encoder_weights=encoder_weights,\n",
    "                classes=len(classes),\n",
    "                activation=activation,\n",
    "                **kwargs,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name {model}\")\n",
    "\n",
    "    if model_weights_path is not None:\n",
    "        model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder_name, encoder_weights)\n",
    "\n",
    "    return model, preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb7db36-6daa-4aef-8138-531a2cb111a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cnts(cnts):\n",
    "    if cnts:\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "        for i, cnt in enumerate(cnts):\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 50:\n",
    "                continue\n",
    "            else:\n",
    "                cnts = cnts[:i]\n",
    "                break\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343c9149-62ec-49cf-8da9-b7ede696cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_barcode(masks, target):\n",
    "    \n",
    "    masks = masks.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    detections = []\n",
    "\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = np.uint8(mask.squeeze()*255)\n",
    "        (cnts, _) = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = process_cnts(cnts)\n",
    "        \n",
    "        # find all bboxes\n",
    "        if cnts:\n",
    "            barcodes = []\n",
    "            for cnt in cnts:\n",
    "                coords = list(cv2.boundingRect(cnt))\n",
    "                coords[2] += coords[0]\n",
    "                coords[3] += coords[1]\n",
    "                barcodes.append(coords)\n",
    "            \n",
    "            # append them to list\n",
    "            if target:\n",
    "                dict_ = dict(\n",
    "                    boxes=torch.Tensor(barcodes),\n",
    "                    labels=torch.zeros(len(barcodes))\n",
    "                )\n",
    "            else:                   \n",
    "                dict_ = dict(\n",
    "                    boxes=torch.Tensor(barcodes),\n",
    "                    scores=torch.ones(len(barcodes)) * 0.9,\n",
    "                    labels=torch.zeros(len(barcodes))\n",
    "                )\n",
    "\n",
    "            detections.append(dict_)\n",
    "                \n",
    "\n",
    "        else:\n",
    "            # create empy predictions list\n",
    "            if target:\n",
    "                dict_ = dict(\n",
    "                    boxes=torch.Tensor([]),\n",
    "                    labels=torch.Tensor([])\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                dict_ = dict(\n",
    "                    boxes=torch.Tensor([]),\n",
    "                    scores=torch.Tensor([]),\n",
    "                    labels=torch.Tensor([])\n",
    "                )\n",
    "                \n",
    "            detections.append(dict_)\n",
    "            \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4047c3e8-73fe-4536-896c-ac8ad0bb6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(pl.LightningModule):\n",
    "    def __init__(self, model, classes, lr: float = 1e-3, scheduler_T=1000, is_quant=False) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.is_quant=is_quant\n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "        self.lr = lr\n",
    "        self.scheduler_T = scheduler_T\n",
    "        self.criterion = smp.utils.losses.DiceLoss()\n",
    "\n",
    "        # define metric\n",
    "        self.metrics = torch.nn.ModuleDict(\n",
    "            {\n",
    "                \"IOUScore\": IoU(num_classes=2),\n",
    "                \"mAP\": MeanAveragePrecision(box_format=\"xyxy\")\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "        batch_idx\n",
    "    ) -> torch.Tensor:\n",
    "        images, target_masks = batch\n",
    "        predicted_masks = self.model(images.float())\n",
    "        loss = self.criterion(predicted_masks, target_masks // 255)\n",
    "        bboxes_target = detect_barcode(target_masks // 255, target=True)\n",
    "        bboxes_predicted = detect_barcode(predicted_masks, target=False)\n",
    "            \n",
    "        for i, metric in enumerate(self.metrics.values()):\n",
    "            if i == 1:\n",
    "                metric.update(bboxes_predicted, bboxes_target)\n",
    "            else:\n",
    "                metric.update(predicted_masks, target_masks // 255)\n",
    "\n",
    "        self.log(\"Train/Loss\", loss.item(), on_step=True, batch_size=BATCH_SIZE)\n",
    "        self.log(\n",
    "            \"Train/LR\",\n",
    "            self.lr_schedulers().get_last_lr()[0],\n",
    "            on_step=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        images, target_masks = batch\n",
    "\n",
    "        predicted_masks = self.model(images.float())\n",
    "        loss = self.criterion(predicted_masks, target_masks // 255)\n",
    "        \n",
    "        bboxes_target = detect_barcode(target_masks // 255, target=True)\n",
    "        bboxes_predicted = detect_barcode(predicted_masks, target=False)        \n",
    "\n",
    "        for i, metric in enumerate(self.metrics.values()):\n",
    "            if i == 1:\n",
    "                metric.update(bboxes_predicted, bboxes_target)\n",
    "            else:\n",
    "                metric.update(predicted_masks, target_masks // 255)\n",
    "\n",
    "        self.log(\n",
    "            \"Validation/Classification Loss\",\n",
    "            loss.item(),\n",
    "            on_step=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, something) -> None:\n",
    "        # print(\"validation_epoch_end\\n\", something, \"\\n\")\n",
    "        print(\"\\nValidation End \\n\", \"******\"*10,)\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric_val = metric.compute()\n",
    "            self.log(f\"Validation/{name}\", metric_val, on_step=False, on_epoch=True)\n",
    "            metric.reset()\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        images, target_masks = batch\n",
    "        images = images.float()\n",
    "        if self.is_quant:\n",
    "            images = self.quant(images)\n",
    "        predicted_masks = self.model(images)\n",
    "        if self.is_quant:\n",
    "            predicted_masks = self.dequant(predicted_masks)\n",
    "        loss = self.criterion(predicted_masks, target_masks // 255)\n",
    "\n",
    "        bboxes_target = detect_barcode(target_masks // 255, target=True) \n",
    "        bboxes_predicted = detect_barcode(predicted_masks, target=False)\n",
    "\n",
    "        for i, metric in enumerate(self.metrics.values()):\n",
    "\n",
    "            if i == 1:\n",
    "                metric.update(bboxes_predicted, bboxes_target)\n",
    "            else:\n",
    "                metric.update(predicted_masks, target_masks // 255)\n",
    "\n",
    "        self.log(\n",
    "            \"Test/Classification Loss\",\n",
    "            loss.item(),\n",
    "            on_step=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_epoch_end(self, something) -> None:\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric_val = metric.compute()\n",
    "            self.log(f\"Test/{name}\", metric_val, on_step=False, on_epoch=True)\n",
    "            metric.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(filter(lambda p: p.requires_grad, self.model.parameters()))\n",
    "        optimizer = torch.optim.Adam(params, lr=self.lr)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, T_max=self.scheduler_T, eta_min=1e-8\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler},\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc9a54-af1d-4255-a727-f0f0c0f2573e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d95186d-b930-4c4d-838b-4ff07ea99e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet, preprocessing_fn_unet = BarcodeSegmentation(model=\"UnetPlusPlus\")\n",
    "preprocessing_unet = get_preprocessing(preprocessing_fn_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8cb49e-ce22-4404-9785-fdf4151d05f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_train:  [PosixPath('Barcodes_v1.1_merged/train/Images/0011210009585_1.jpg')]\n",
      "masks_train:  [PosixPath('Barcodes_v1.1_merged/train/Mask/0_0_0.png')]\n",
      "is len of train images == len of train masks: True\n",
      "is len of val images == len of val masks: True\n",
      "is len of test images == len of test masks: True\n"
     ]
    }
   ],
   "source": [
    "# gather paths to images\n",
    "dataset_folder_to_use = Path(\"./Barcodes_v1.1_merged\")\n",
    "\n",
    "images_train = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"train\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "images_val = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"val\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "images_test = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"test\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "print(\"images_train: \", images_train[:1])\n",
    "\n",
    "masks_train = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"train\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "masks_val = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"val\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "masks_test = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"test\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "print(\"masks_train: \", masks_train[:1])\n",
    "\n",
    "check_trian = [x.stem for x in masks_train]\n",
    "check_val = [x.stem for x in masks_val]\n",
    "check_test = [x.stem for x in masks_test]\n",
    "\n",
    "images_train = [x for x in images_train if x.stem in check_trian]\n",
    "images_val = [x for x in images_val if x.stem in check_val]\n",
    "images_test = [x for x in images_test if x.stem in check_test]\n",
    "\n",
    "\n",
    "print(f\"is len of train images == len of train masks: {len(images_train) == len(masks_train)}\")\n",
    "print(f\"is len of val images == len of val masks: {len(images_train) == len(masks_train)}\")\n",
    "print(f\"is len of test images == len of test masks: {len(images_train) == len(masks_train)}\")\n",
    "\n",
    "for img, msk in zip(images_train, masks_train):\n",
    "    if Path(img).stem != Path(msk).stem:\n",
    "        print(\"Error!\")\n",
    "        raise AssertionError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc7da67-414a-4e4b-a564-5781063691d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataSet(\n",
    "    images=images_train,\n",
    "    masks=masks_train,\n",
    "    transform=Transforms(),\n",
    "    preprocessing=preprocessing_unet,\n",
    ")\n",
    "val_dataset = SegmentationDataSet(\n",
    "    images=images_val,\n",
    "    masks=masks_val,\n",
    "    transform=Transforms(segment=\"val\"),\n",
    "    preprocessing=preprocessing_unet,\n",
    ")\n",
    "test_dataset = SegmentationDataSet(\n",
    "    images=images_test,\n",
    "    masks=masks_test,\n",
    "    transform=Transforms(segment=\"val\"),\n",
    "    preprocessing=preprocessing_unet\n",
    ")\n",
    "\n",
    "train_dl_unet = DataLoader(\n",
    "    train_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dl_unet = DataLoader(\n",
    "    val_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dl_unet = DataLoader(\n",
    "    test_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c08a7-0aae-4ebd-bada-fd7122ee63c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a325d06-6d41-4bc8-b4cc-6ecb1bf91b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt = torch.load(\"./checkpoint-UnetPlusPlus-epoch=09.ckpt\")\n",
    "\n",
    "prefix = 'model.'\n",
    "n_clip = len(prefix)\n",
    "adapted_chkpt = {k[n_clip:]: v for k, v in chkpt[\"state_dict\"].items() if k.startswith(prefix)}\n",
    "model_unet.load_state_dict(adapted_chkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a996c94-cc60-48d7-af63-a8f70d8a4584",
   "metadata": {},
   "source": [
    "# Model Metrics Before Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dea51f8-70e9-4b07-8711-90e9f5248c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4dad279-802f-45ea-aca0-e12cad5e171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_unet = Runner(\n",
    "    model=model_unet,\n",
    "    classes=[\"barcode\"],\n",
    "    lr=1e-3,\n",
    "    scheduler_T=max_epochs * len(train_dl_unet)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d04553b5-315f-4cb0-8135-312639edf025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3f4901ca37428fb0a974d4f8f28705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.053776539862155914,\n",
      " 'Test/Classification Loss_epoch': 0.053776539862155914,\n",
      " 'Test/IOUScore': 0.9542956948280334,\n",
      " 'Test/mAP': {'map': tensor(0.5806, device='cuda:0'),\n",
      "              'map_50': tensor(0.8760, device='cuda:0'),\n",
      "              'map_75': tensor(0.6039, device='cuda:0'),\n",
      "              'map_large': tensor(0.9222, device='cuda:0'),\n",
      "              'map_medium': tensor(0.7587, device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(0.3731, device='cuda:0'),\n",
      "              'mar_1': tensor(0.6283, device='cuda:0'),\n",
      "              'mar_10': tensor(0.7152, device='cuda:0'),\n",
      "              'mar_100': tensor(0.7152, device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(0.9500, device='cuda:0'),\n",
      "              'mar_medium': tensor(0.8123, device='cuda:0'),\n",
      "              'mar_small': tensor(0.4759, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=-1)\n",
    "score = trainer.test(runner_unet, dataloaders=val_dl_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc45d8-d698-434d-916d-e67004d58c10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Global pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f442ba-6e46-4931-90fe-2cde442c6855",
   "metadata": {},
   "source": [
    "\"The point of PyTorch pruning, at the moment, is not necessarily to guarantee inference time speedups or memory savings.  \n",
    "It’s more of an experimental feature to enable pruning research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38f9dea0-e8c0-4c6b-b29c-23be2d87d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_global_unstructured(model, layer_type, proportion):\n",
    "    module_tups = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, layer_type):\n",
    "            module_tups.append((module, 'weight'))\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters=module_tups, pruning_method=prune.L1Unstructured,\n",
    "        amount=proportion\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def remove_params(model, layer_type):\n",
    "    module_tups = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, layer_type):\n",
    "            module_tups.append((module, 'weight'))\n",
    "            \n",
    "    for module, _ in module_tups:\n",
    "        prune.remove(module, 'weight')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e927c24c-1cfd-4bc1-aa59-2ec2013bb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8ad4462-1a1d-4f97-a6ae-351051f6d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_global_sparsity(model,\n",
    "                            weight=True,\n",
    "                            bias=False,\n",
    "                            conv2d_use_mask=False,\n",
    "                            linear_use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d70d6be-a7d4-41be-a9ac-4460092cf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_pruning_finetuning(\n",
    "    model=runner_unet,\n",
    "    train_loader=train_dl_unet,\n",
    "    test_loader=val_dl_unet,\n",
    "    conv2d_prune_proportion=0.1,\n",
    "    num_iterations=2,\n",
    "    num_epochs_per_iteration=2,\n",
    "    fntn=False\n",
    "):\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=-1, \n",
    "        max_epochs=num_epochs_per_iteration\n",
    "    )\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        model = model.cuda()\n",
    "        print(\"\\n\\n\", \"=========\"*5)\n",
    "        print(\"Pruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n",
    "\n",
    "        print(\"Pruning...\")\n",
    "        model = prune_model_global_unstructured(model=model,\n",
    "                                                layer_type=nn.Conv2d, \n",
    "                                                proportion=conv2d_prune_proportion)\n",
    "        \n",
    "        print(\"Measure the quality after Pruning...\")\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model,\n",
    "            weight=True,\n",
    "            bias=False,\n",
    "            conv2d_use_mask=True,\n",
    "            linear_use_mask=False)\n",
    "        print(f\"Global Sparsity: {sparsity:0.02}\")\n",
    "        score = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "        if fntn:\n",
    "            print(\"Fine-tuning the model...\")\n",
    "            # trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "            print(\"Measure the quality after Fine-tuning...\")\n",
    "            num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "                model,\n",
    "                weight=True,\n",
    "                bias=False,\n",
    "                conv2d_use_mask=True,\n",
    "                linear_use_mask=False)\n",
    "            print(f\"Global Sparsity: {sparsity:0.02}\")\n",
    "            score = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "\n",
    "        \n",
    "    # remove params\n",
    "    model = remove_params(model, layer_type=nn.Conv2d)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cc46bfb-1655-4804-8598-f654765fcc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============================================\n",
      "Pruning and Finetuning 1/5\n",
      "Pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure the quality after Pruning...\n",
      "Global Sparsity: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7531a952c92f4ef29861e48587e17194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.05466263368725777,\n",
      " 'Test/Classification Loss_epoch': 0.05466263368725777,\n",
      " 'Test/IOUScore': 0.9534895420074463,\n",
      " 'Test/mAP': {'map': tensor(0.5792, device='cuda:0'),\n",
      "              'map_50': tensor(0.9027, device='cuda:0'),\n",
      "              'map_75': tensor(0.6047, device='cuda:0'),\n",
      "              'map_large': tensor(0.9045, device='cuda:0'),\n",
      "              'map_medium': tensor(0.7604, device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(0.3491, device='cuda:0'),\n",
      "              'mar_1': tensor(0.6299, device='cuda:0'),\n",
      "              'mar_10': tensor(0.7114, device='cuda:0'),\n",
      "              'mar_100': tensor(0.7114, device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(0.9500, device='cuda:0'),\n",
      "              'mar_medium': tensor(0.8132, device='cuda:0'),\n",
      "              'mar_small': tensor(0.4621, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============================================\n",
      "Pruning and Finetuning 2/5\n",
      "Pruning...\n",
      "Measure the quality after Pruning...\n",
      "Global Sparsity: 0.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a504619f4b04778b080c3353d6ecdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.06966167688369751,\n",
      " 'Test/Classification Loss_epoch': 0.06966167688369751,\n",
      " 'Test/IOUScore': 0.9441142678260803,\n",
      " 'Test/mAP': {'map': tensor(0.4131, device='cuda:0'),\n",
      "              'map_50': tensor(0.6625, device='cuda:0'),\n",
      "              'map_75': tensor(0.4191, device='cuda:0'),\n",
      "              'map_large': tensor(0.7236, device='cuda:0'),\n",
      "              'map_medium': tensor(0.6279, device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(0.2684, device='cuda:0'),\n",
      "              'mar_1': tensor(0.5913, device='cuda:0'),\n",
      "              'mar_10': tensor(0.6989, device='cuda:0'),\n",
      "              'mar_100': tensor(0.6989, device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(0.9667, device='cuda:0'),\n",
      "              'mar_medium': tensor(0.8000, device='cuda:0'),\n",
      "              'mar_small': tensor(0.4448, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============================================\n",
      "Pruning and Finetuning 3/5\n",
      "Pruning...\n",
      "Measure the quality after Pruning...\n",
      "Global Sparsity: 0.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf49c6329bc49acbc3a2c538c11916f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.6694976687431335,\n",
      " 'Test/Classification Loss_epoch': 0.6694976687431335,\n",
      " 'Test/IOUScore': 0.562198281288147,\n",
      " 'Test/mAP': {'map': tensor(0.1759, device='cuda:0'),\n",
      "              'map_50': tensor(0.3553, device='cuda:0'),\n",
      "              'map_75': tensor(0.1496, device='cuda:0'),\n",
      "              'map_large': tensor(0.1366, device='cuda:0'),\n",
      "              'map_medium': tensor(0.3010, device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(0.1539, device='cuda:0'),\n",
      "              'mar_1': tensor(0.3515, device='cuda:0'),\n",
      "              'mar_10': tensor(0.3770, device='cuda:0'),\n",
      "              'mar_100': tensor(0.3770, device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(0.1333, device='cuda:0'),\n",
      "              'mar_medium': tensor(0.4170, device='cuda:0'),\n",
      "              'mar_small': tensor(0.3360, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============================================\n",
      "Pruning and Finetuning 4/5\n",
      "Pruning...\n",
      "Measure the quality after Pruning...\n",
      "Global Sparsity: 0.76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72eab2818a3428e82a83d90492f530b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.9994770288467407,\n",
      " 'Test/Classification Loss_epoch': 0.9994770288467407,\n",
      " 'Test/IOUScore': 0.46478089690208435,\n",
      " 'Test/mAP': {'map': tensor(0., device='cuda:0'),\n",
      "              'map_50': tensor(0., device='cuda:0'),\n",
      "              'map_75': tensor(0., device='cuda:0'),\n",
      "              'map_large': tensor(-1., device='cuda:0'),\n",
      "              'map_medium': tensor(0., device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(-1., device='cuda:0'),\n",
      "              'mar_1': tensor(0., device='cuda:0'),\n",
      "              'mar_10': tensor(0., device='cuda:0'),\n",
      "              'mar_100': tensor(0., device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(-1., device='cuda:0'),\n",
      "              'mar_medium': tensor(0., device='cuda:0'),\n",
      "              'mar_small': tensor(-1., device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============================================\n",
      "Pruning and Finetuning 5/5\n",
      "Pruning...\n",
      "Measure the quality after Pruning...\n",
      "Global Sparsity: 0.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be673887d7d4f4e97b6d04642b37b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.9992843866348267,\n",
      " 'Test/Classification Loss_epoch': 0.9992843866348267,\n",
      " 'Test/IOUScore': 0.46478089690208435,\n",
      " 'Test/mAP': {'map': tensor(-1., device='cuda:0'),\n",
      "              'map_50': tensor(-1., device='cuda:0'),\n",
      "              'map_75': tensor(-1., device='cuda:0'),\n",
      "              'map_large': tensor(-1., device='cuda:0'),\n",
      "              'map_medium': tensor(-1., device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(-1., device='cuda:0'),\n",
      "              'mar_1': tensor(-1., device='cuda:0'),\n",
      "              'mar_10': tensor(-1., device='cuda:0'),\n",
      "              'mar_100': tensor(-1., device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(-1., device='cuda:0'),\n",
      "              'mar_medium': tensor(-1., device='cuda:0'),\n",
      "              'mar_small': tensor(-1., device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_pruned = iterative_pruning_finetuning(\n",
    "    model=runner_unet,\n",
    "    train_loader=train_dl_unet,\n",
    "    test_loader=val_dl_unet,\n",
    "    conv2d_prune_proportion=0.3,\n",
    "    num_iterations=5,\n",
    "    num_epochs_per_iteration=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96399cd-1690-43bd-bd55-93375166a01c",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaee74-9821-466e-88c4-3d6c79039c42",
   "metadata": {},
   "source": [
    "| Sparsity level      | IoU Score |\n",
    "| :-----------: | :-----------: |\n",
    "| 0.3      | 0.95       |\n",
    "| 0.51   | 0.945        |\n",
    "| 0.66   | 0.56        |\n",
    "| 0.76   | 0.46        |\n",
    "| 0.83   | 0.46        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a4e18-52b3-421f-9fd9-d0d0d805b086",
   "metadata": {},
   "source": [
    "As can be seen from the table, we can zero out about 50% of the weights in the convolutional layers of the model without significant loss of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751afa7-1fd0-4c58-b7fa-244ccba921f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4f46b96-c228-45fe-af70-e84691a4ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_unet_fp32, preprocessing_fn_unet = BarcodeSegmentation(model=\"UnetPlusPlus\")\n",
    "preprocessing_unet = get_preprocessing(preprocessing_fn_unet)\n",
    "\n",
    "chkpt = torch.load(\"./checkpoint-UnetPlusPlus-epoch=09.ckpt\")\n",
    "\n",
    "prefix = 'model.'\n",
    "n_clip = len(prefix)\n",
    "adapted_chkpt = {k[n_clip:]: v for k, v in chkpt[\"state_dict\"].items() if k.startswith(prefix)}\n",
    "model_unet_fp32.load_state_dict(adapted_chkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8a9a1-7cd0-4774-954a-ec6ab138efc1",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "Quantization is useful when it is required to serve large models on machines with limited memory, or when there’s a need to switch between models and reducing the I/O time is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2267a-bfe0-4780-9550-ecac437c1990",
   "metadata": {},
   "source": [
    "Teoretical reductions:\n",
    "* 2-4x reduction in memory bandwidth\n",
    "* 2-4x faster inference due to savings in memory bandwidth and compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147511d-bbaf-438d-bf1d-7fe299570ac3",
   "metadata": {},
   "source": [
    "https://spell.ml/blog/pytorch-quantization-X8e7wBAAACIAHPhT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d22744-dc98-4883-a8ec-a9638cd1c4aa",
   "metadata": {},
   "source": [
    "## Dynamic quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce067aae-311b-41e9-806e-5c328c3ba62c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "An important limitation of Dynamic Quantization, while it is the easiest workflow if you do not have a pre-trained quantized model ready for use, is that it currently only supports nn.Linear and nn.LSTM in qconfig_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1bb364d-0997-4466-ad5c-dfbef742fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_unet, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18024b3-f02a-488b-a646-480c360bddc4",
   "metadata": {},
   "source": [
    "So, this type of quantization is absolutely useless for the segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9db92-5cd3-4c97-9e6e-7d1445923087",
   "metadata": {},
   "source": [
    "## Static quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82ccf-bbb9-424e-b719-e9073adb54b5",
   "metadata": {},
   "source": [
    "Static quantization works by fine-tuning the quantization algorithm on a test dataset after initial model training is complete. This additional scoring process is not used to fine-tune the model—only to adjust the quantization algorithm parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2499a5-a85b-478c-a26e-0833177af3b1",
   "metadata": {},
   "source": [
    "Еo get the most performance out of static quantization, you need to also use module fusion. Module fusion is the technique of combining (\"fusing\") sequences of high-level layers, e.g. Conv2d + Batchnorm, into a single combined layer. \n",
    "This improves performance by pushing the combined sequence of operations into the low-level library, allowing it to be computed in one shot, e.g. without having to surface an intermediate representation back to the PyTorch Python process. This speeds things up and leads to more accurate results, albeit at the cost of debuggability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696e5cd-e9ca-48ac-8c7c-2bcac5c11864",
   "metadata": {},
   "source": [
    "### Pytorch Lighting quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88688d39-b4a3-4f5d-a9c1-6a87866daa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet_fp32 = Runner(\n",
    "    model=model_unet_fp32.eval(),\n",
    "    classes=[\"barcode\"],\n",
    "    is_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d52abb0-8880-4b6c-8fff-9a547cd75cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantStub converts tensors from floating point to quantized\n",
    "model_unet_fp32.quant = torch.quantization.QuantStub()\n",
    "# DeQuantStub converts tensors from quantized to floating point\n",
    "model_unet_fp32.dequant = torch.quantization.DeQuantStub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0bd8d57e-1f8e-4ff3-908c-c10cfda156d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify forward pass\n",
    "# model_unet_fp32.forward = wrap_quantize_forward_context(model=model_unet_fp32, func=model_unet_fp32.forward)\n",
    "\n",
    "# def wrap_quantize_forward_context(model: \"pl.LightningModule\", func):\n",
    "#     \"\"\"Decorator to wrap forward path as it is needed to quantize inputs and dequantize outputs for in/out\n",
    "#     compatibility.\"\"\"\n",
    "#     # todo: consider using registering hook before/after forward\n",
    "#     @functools.wraps(func)\n",
    "#     def wrapper(data):\n",
    "#         data = model.quant(data)\n",
    "#         data = func(data)\n",
    "#         data = model.dequant(data)\n",
    "#         return data\n",
    "\n",
    "#     return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bfdbe33-64b6-4cee-8700-8446c851e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config\n",
    "model_unet_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "599aadec-4e68-4d0d-9331-505972556fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model fuze \n",
    "\n",
    "# from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
    "\n",
    "# modules_to_fuse=[['_blocks']]\n",
    "\n",
    "# def _recursive_hasattr(obj, attribs: str, state: bool = True) -> bool:\n",
    "#     \"\"\"recursive check if model has some layers denoted with '.'.\"\"\"\n",
    "#     print(attribs)\n",
    "#     if \".\" in attribs:\n",
    "#         attrib, attribs = attribs.split(\".\", 1)\n",
    "#         if hasattr(obj, attrib):\n",
    "#             return _recursive_hasattr(getattr(obj, attrib), attribs, state)\n",
    "#         return False\n",
    "#     return state and hasattr(obj, attribs)\n",
    "\n",
    "# def _check_feasible_fuse(model: \"pl.LightningModule\") -> bool:\n",
    "#     if not modules_to_fuse:\n",
    "#         return False\n",
    "#     for group in modules_to_fuse:\n",
    "#         if not all(_recursive_hasattr(model, m) for m in group):\n",
    "#             raise MisconfigurationException(\n",
    "#                 f\"You have requested to fuse {group} but one or more of them is not your model attributes\"\n",
    "#             )\n",
    "            \n",
    "# _check_feasible_fuse(model=model_unet_fp32.model.encoder)\n",
    "\n",
    "# # fuse modules\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(model_unet_fp32.model, [['_conv_stem', 'BatchNorm2d']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9a9e07b-5459-49e3-b7b1-c3f158637b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model_unet_fp32_prepared = torch.quantization.prepare(model_unet_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc47ce-b221-4eb2-9d30-a2b59c9c1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_static = pl.Trainer(\n",
    "    accelerator=\"cpu\"\n",
    ")\n",
    "\n",
    "metrics = trainer_static.test(model_unet_fp32_prepared, val_dl_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ef12e45-fdaa-47a2-8c7f-e0b0f9e0a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model to model_int8\n",
    "model_int8_static = torch.quantization.convert(model_unet_fp32_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba776662-9d18-4dd9-af13-2e519e4409df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n"
     ]
    }
   ],
   "source": [
    "print(model_int8_static.quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8280bb58-29b2-4114-b699-ac2e69f86ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::thnn_conv2d_forward' is only available for these backends: [CPU, CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCPU.cpp:16286 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1590/1909518887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl_unet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_int8_static\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_int8_static\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/encoders/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Identity and Sequential stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Block stages need drop_connect rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::thnn_conv2d_forward' is only available for these backends: [CPU, CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCPU.cpp:16286 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "batch = iter(val_dl_unet).next()[0][0].squeeze().float()\n",
    "model_int8_static.model(model_int8_static.quant(batch[None, ...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "609d6f5b-01f0-478e-a293-19369f2d7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_static.test(model_int8_static, val_dl_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a713e3a-b2eb-4965-acf1-8e14c3dcdd0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pure pytorch quantization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed54d4-68ca-4b40-919f-785801b60139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quant and dequant modules to the model\n",
    "model_unet_fp32.add_module(name=\"quant\", module=torch.quantization.QuantStub())\n",
    "model_unet_fp32.add_module(name=\"dequant\", module=torch.quantization.DeQuantStub())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c653e9d-2739-4767-9d87-4aac531e4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the backend on which the quantized kernels need to be run\n",
    "torch.backends.quantized.engine='fbgemm'\n",
    "\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_unet_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1cdda-0164-4f78-8cce-0dbd7e8f60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_unet_fp32 = torch.quantization.fuse_modules(model_unet_fp32, [['conv', 'relu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c35bf69-379c-4f95-954b-dbd5fdaffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_unet_fp32_prepared = torch.quantization.prepare(model_unet_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644c161-60c1-4eff-b1c2-a0d0f46d02e4",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html  \n",
    "https://pytorch.org/blog/introduction-to-quantization-on-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ff27d-920b-43fa-8082-83e4ea66e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "model_unet_fp32_prepared(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eddbe6c-c18c-4865-96f9-18a488946d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_unet_fp32_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7e8db-ab47-4242-8e22-6e506ca91df2",
   "metadata": {},
   "source": [
    "## QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1fe49-6639-4b8c-ba20-0e2115d55111",
   "metadata": {},
   "source": [
    "Quantization Aware Training (QAT) mimics the effects of quantization during training: The computations are carried-out in floating-point precision but the subsequent quantization effect is taken into account. The weights and activations are quantized into lower precision only for inference, when training is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d027603-2821-45fa-b3eb-4058af0744d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pure pytorch qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7d3b0-8d18-4f68-a4dd-2e523888dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify quantization config for QAT\n",
    "qat_model.qconfig=torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# prepare QAT\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "# run training\n",
    "\n",
    "# convert to quantized version, removing dropout, to check for accuracy on each\n",
    "epochquantized_model=torch.quantization.convert(qat_model.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af501c-e3de-4153-b382-3c796f06d86e",
   "metadata": {},
   "source": [
    "### Pytorch Lighting QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fec62440-2a71-4acf-99b6-13f0640467ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "qmodel = Runner(\n",
    "    model=model_unet_fp32,\n",
    "    classes=[\"barcode\"],\n",
    "    lr=1e-5,\n",
    "    scheduler_T=3 * len(train_dl_unet),\n",
    ")\n",
    "\n",
    "\n",
    "qcb = QuantizationAwareTraining(\n",
    "    quantize_on_fit_end=True,\n",
    "    input_compatible=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer_unet = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    gpus=-1,\n",
    "    callbacks=[qcb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1c2c687-0266-429a-a15d-c3e6aa3c9ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | model     | UnetPlusPlus | 13.7 M\n",
      "1 | criterion | DiceLoss     | 0     \n",
      "2 | metrics   | ModuleDict   | 0     \n",
      "3 | quant     | QuantStub    | 0     \n",
      "4 | dequant   | DeQuantStub  | 0     \n",
      "-------------------------------------------\n",
      "13.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.7 M    Total params\n",
      "54.861    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eb8753147d4241b681a0b62a2bb32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_unet.fit(qmodel, train_dl_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e09077b-b11b-42e2-b247-cb5d7b750098",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::thnn_conv2d_forward' is only available for these backends: [CPU, CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCPU.cpp:16286 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1590/3015532869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl_unet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/segmentation_models_pytorch/encoders/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Identity and Sequential stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Block stages need drop_connect rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::thnn_conv2d_forward' is only available for these backends: [CPU, CUDA, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCPU.cpp:16286 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/build/aten/src/ATen/RegisterCUDA.cpp:20674 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/core/VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/VariableType_0.cpp:9848 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/autograd/generated/TraceType_0.cpp:9750 [kernel]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/autocast_mode.cpp:255 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "batch = iter(val_dl_unet).next()[0][0].squeeze().float()\n",
    "qmodel.model(qmodel.quant(batch[None, ...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95c9d1-669e-4ee5-83a1-074badef7596",
   "metadata": {},
   "source": [
    "# OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5baf4d49-e7de-4232-9ff2-44f754df2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openvino\n",
    "# !pip install nncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24d4bb5-b410-400d-9270-a9a89767ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "fp32_onnx_path = \"./model_unet_fp32.onnx\"\n",
    "int8_onnx_path = \"./model_unet_int8.onnx\"\n",
    "OUTPUT_DIR = \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ba4073-4537-46b5-b729-311445fc77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d93b21-bd74-4967-b701-28f1c8fc40f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_unet_fp32, preprocessing_fn_unet = BarcodeSegmentation(model=\"UnetPlusPlus\")\n",
    "preprocessing_unet = get_preprocessing(preprocessing_fn_unet)\n",
    "\n",
    "chkpt = torch.load(\"./checkpoint-UnetPlusPlus-epoch=09.ckpt\")\n",
    "\n",
    "prefix = 'model.'\n",
    "n_clip = len(prefix)\n",
    "adapted_chkpt = {k[n_clip:]: v for k, v in chkpt[\"state_dict\"].items() if k.startswith(prefix)}\n",
    "model_unet_fp32.load_state_dict(adapted_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e0cf37-3017-4d37-9344-cfce15293f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_train:  [PosixPath('Barcodes_v1.1_merged/train/Images/0011210009585_1.jpg')]\n",
      "masks_train:  [PosixPath('Barcodes_v1.1_merged/train/Mask/0_0_0.png')]\n",
      "is len of train images == len of train masks: True\n",
      "is len of val images == len of val masks: True\n",
      "is len of test images == len of test masks: True\n"
     ]
    }
   ],
   "source": [
    "# gather paths to images\n",
    "dataset_folder_to_use = Path(\"./Barcodes_v1.1_merged\")\n",
    "\n",
    "images_train = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"train\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "images_val = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"val\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "images_test = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"test\" / \"Images\").glob(\"*\")\n",
    "])\n",
    "print(\"images_train: \", images_train[:1])\n",
    "\n",
    "masks_train = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"train\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "masks_val = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"val\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "masks_test = sorted([\n",
    "    file for file in (dataset_folder_to_use / \"test\" / \"Mask\").glob(\"*\")\n",
    "])\n",
    "print(\"masks_train: \", masks_train[:1])\n",
    "\n",
    "check_trian = [x.stem for x in masks_train]\n",
    "check_val = [x.stem for x in masks_val]\n",
    "check_test = [x.stem for x in masks_test]\n",
    "\n",
    "images_train = [x for x in images_train if x.stem in check_trian]\n",
    "images_val = [x for x in images_val if x.stem in check_val]\n",
    "images_test = [x for x in images_test if x.stem in check_test]\n",
    "\n",
    "\n",
    "print(f\"is len of train images == len of train masks: {len(images_train) == len(masks_train)}\")\n",
    "print(f\"is len of val images == len of val masks: {len(images_train) == len(masks_train)}\")\n",
    "print(f\"is len of test images == len of test masks: {len(images_train) == len(masks_train)}\")\n",
    "\n",
    "for img, msk in zip(images_train, masks_train):\n",
    "    if Path(img).stem != Path(msk).stem:\n",
    "        print(\"Error!\")\n",
    "        raise AssertionError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9021df91-6588-476c-88b7-c7d6f794a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataSet(\n",
    "    images=images_train,\n",
    "    masks=masks_train,\n",
    "    transform=Transforms(),\n",
    "    preprocessing=preprocessing_unet,\n",
    ")\n",
    "val_dataset = SegmentationDataSet(\n",
    "    images=images_val,\n",
    "    masks=masks_val,\n",
    "    transform=Transforms(segment=\"val\"),\n",
    "    preprocessing=preprocessing_unet,\n",
    ")\n",
    "test_dataset = SegmentationDataSet(\n",
    "    images=images_test,\n",
    "    masks=masks_test,\n",
    "    transform=Transforms(segment=\"val\"),\n",
    "    preprocessing=preprocessing_unet\n",
    ")\n",
    "\n",
    "train_dl_unet = DataLoader(\n",
    "    train_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dl_unet = DataLoader(\n",
    "    val_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dl_unet = DataLoader(\n",
    "    test_dataset,\n",
    "    BATCH_SIZE,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16fb82c-52eb-40d2-a5c3-4afe8aaf65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbd21807a844c4f8ac3c3f1ac143cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.0664256140589714,\n",
      " 'Test/Classification Loss_epoch': 0.0664256140589714,\n",
      " 'Test/IOUScore': 0.9522693157196045,\n",
      " 'Test/mAP': {'map': tensor(0.5790, device='cuda:0'),\n",
      "              'map_50': tensor(0.8629, device='cuda:0'),\n",
      "              'map_75': tensor(0.6746, device='cuda:0'),\n",
      "              'map_large': tensor(0.7689, device='cuda:0'),\n",
      "              'map_medium': tensor(0.7370, device='cuda:0'),\n",
      "              'map_per_class': tensor(-1., device='cuda:0'),\n",
      "              'map_small': tensor(0.4314, device='cuda:0'),\n",
      "              'mar_1': tensor(0.6757, device='cuda:0'),\n",
      "              'mar_10': tensor(0.7186, device='cuda:0'),\n",
      "              'mar_100': tensor(0.7186, device='cuda:0'),\n",
      "              'mar_100_per_class': tensor(-1., device='cuda:0'),\n",
      "              'mar_large': tensor(0.8727, device='cuda:0'),\n",
      "              'mar_medium': tensor(0.8234, device='cuda:0'),\n",
      "              'mar_small': tensor(0.5000, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "runner_unet = Runner(\n",
    "    model=model_unet_fp32,\n",
    "    classes=[\"barcode\"],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=-1)\n",
    "score = trainer.test(runner_unet, dataloaders=test_dl_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e71d56-0a7d-4524-b178-0dd94f5956fa",
   "metadata": {},
   "source": [
    "Export the FP32 model to ONNX, which is supported by OpenVINO™ Toolkit, to benchmark it in comparison with the INT8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ffc5a7-b576-4324-8648-04e139735b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, *IMAGE_SIZE).to(device)\n",
    "model_unet_fp32.encoder.set_swish(memory_efficient=False)\n",
    "model_unet_fp32 = model_unet_fp32.to(device)\n",
    "torch.onnx.export(model_unet_fp32, dummy_input, fp32_onnx_path, opset_version=10)\n",
    "print(f\"FP32 ONNX model was exported to {fp32_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081e6e0-57f3-4082-90b1-fc08e7e5e67e",
   "metadata": {},
   "source": [
    "Configure NNCF parameters to specify compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c25ff39-a339-4510-9a46-24863e93ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 3, *IMAGE_SIZE]},\n",
    "    \"log_dir\": str(OUTPUT_DIR),\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "        \"initializer\": {\n",
    "            \"range\": {\"num_init_samples\": 15000},\n",
    "            \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": 4000},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9922c286-02da-4610-baa1-01bfe6c38673",
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config = register_default_init_args(nncf_config, train_dl_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498099c-98aa-491c-81de-fc3a8d71df80",
   "metadata": {},
   "source": [
    "Create a quantized model from a pre-trained FP32 model and configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dffe5139-dc7c-4703-a712-d97bb26dbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ctrl, model = create_compressed_model(model_unet_fp32, nncf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d482b82a-8b60-4ce3-998b-e2f42dfa4f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b32aeeb530446e9142d58776de05eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test/Classification Loss': 0.0980178639292717,\n",
      " 'Test/Classification Loss_epoch': 0.0980178639292717,\n",
      " 'Test/IOUScore': 0.9451008439064026,\n",
      " 'Test/mAP': {'map': tensor(0.6616),\n",
      "              'map_50': tensor(0.8492),\n",
      "              'map_75': tensor(0.7644),\n",
      "              'map_large': tensor(0.8664),\n",
      "              'map_medium': tensor(0.8352),\n",
      "              'map_per_class': tensor(-1.),\n",
      "              'map_small': tensor(0.4125),\n",
      "              'mar_1': tensor(0.7536),\n",
      "              'mar_10': tensor(0.7810),\n",
      "              'mar_100': tensor(0.7810),\n",
      "              'mar_100_per_class': tensor(-1.),\n",
      "              'mar_large': tensor(0.9091),\n",
      "              'mar_medium': tensor(0.9019),\n",
      "              'mar_small': tensor(0.4940)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "runner_unet = Runner(\n",
    "    model=model,\n",
    "    classes=[\"barcode\"],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"cpu\")\n",
    "score = trainer.test(runner_unet, dataloaders=test_dl_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2029e6-7c0b-4134-b334-a062bec2e711",
   "metadata": {},
   "source": [
    "**IOUScore of fp32 model**: 0.9523  \n",
    "**IOUScore of int8 model**: 0.9451"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe400c-ce17-47b5-a54d-99049b7c63e3",
   "metadata": {},
   "source": [
    "Export INT8 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f178eed7-ae56-473e-bdcf-7bf0d9049583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 ONNX model exported to ./model_unet_int8.onnx.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)  # Ignore export warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "compression_ctrl.export_model(int8_onnx_path)\n",
    "print(f\"INT8 ONNX model exported to {int8_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab476561-1fec-4a57-89f9-858ce8dfdda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 224, 224]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = [1, 3, *IMAGE_SIZE]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08c6cdd9-07f8-43ec-8448-1dd664464cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$fp32_onnx_path'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_onnx_path = \"./model_unet_fp32.onnx\"\n",
    "int8_onnx_path = \"./model_unet_int8.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5855920d-87bd-4513-b299-a6bb00a848cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x7f0fd8a28950>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_fn_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad9b9d13-2503-4809-a5b4-ed286f5f4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/jupyter/barcodes/segnet/model_unet_fp32.onnx\n",
      "\t- Path for generated IR: \t/home/jupyter/barcodes/segnet/output/fp32\n",
      "\t- IR output name: \tmodel_unet_fp32\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1, 3, 224, 224]\n",
      "\t- Mean values: \t[123.675, 116.28 , 103.53]\n",
      "\t- Scale values: \t[58.395, 57.12 , 57.375]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/opt/conda/lib/python3.7/site-packages/openvino\n",
      "Inference Engine version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
      "Model Optimizer version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
      "[ WARNING ]  Const node 'Resize_512/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_407/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_459/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_647/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_673/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_539/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_620/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_566/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_433/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_593/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_485/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Changing Const node 'Resize_485/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_459/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_566/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_433/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_539/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_620/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_407/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_512/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_593/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_647/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_673/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/jupyter/barcodes/segnet/output/fp32/model_unet_fp32.xml\n",
      "[ SUCCESS ] BIN file: /home/jupyter/barcodes/segnet/output/fp32/model_unet_fp32.bin\n",
      "[ SUCCESS ] Total execution time: 52.33 seconds. \n",
      "[ SUCCESS ] Memory consumed: 307 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n"
     ]
    }
   ],
   "source": [
    "!mo --input_model \"model_unet_fp32.onnx\" --input_shape \"[1, 3, 224, 224]\" --scale \"255\" --mean_values \"[0.485, 0.456, 0.406]\" --scale_values \"[0.229, 0.224, 0.225]\" --data_type FP16 --output_dir \"output/fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38b80b20-f9cb-42ac-80c5-1ad13b8f42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/jupyter/barcodes/segnet/model_unet_int8.onnx\n",
      "\t- Path for generated IR: \t/home/jupyter/barcodes/segnet/output/int8\n",
      "\t- IR output name: \tmodel_unet_int8\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1, 3, 224, 224]\n",
      "\t- Mean values: \t[123.675, 116.28, 103.53]\n",
      "\t- Scale values: \t[58.395, 57.12, 57.375]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/opt/conda/lib/python3.7/site-packages/openvino\n",
      "Inference Engine version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
      "Model Optimizer version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
      "[ WARNING ]  Const node 'Resize_1309/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1980/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1087/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1756/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_2092/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1420/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1868/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1532/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1644/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_1198/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_2203/Add' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Changing Const node 'Resize_1420/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1309/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1756/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1198/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1644/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1980/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1087/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1532/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_1868/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_2092/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_2203/Add' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/jupyter/barcodes/segnet/output/int8/model_unet_int8.xml\n",
      "[ SUCCESS ] BIN file: /home/jupyter/barcodes/segnet/output/int8/model_unet_int8.bin\n",
      "[ SUCCESS ] Total execution time: 138.79 seconds. \n",
      "[ SUCCESS ] Memory consumed: 396 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n"
     ]
    }
   ],
   "source": [
    "!mo --input_model \"model_unet_int8.onnx\" --input_shape \"[1, 3, 224, 224]\" --scale \"255\" --mean_values \"[0.485, 0.456, 0.406]\" --scale_values \"[0.229, 0.224, 0.225]\" --data_type FP16 --output_dir \"output/int8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28fad-0655-443e-9446-6dc406469755",
   "metadata": {},
   "source": [
    "we will measure the inference performance of the FP32 and INT8 models. To do this, we use Benchmark Tool - OpenVINO’s inference performance measurement tool. By default, Benchmark Tool runs inference for 60 seconds in asynchronous mode on CPU. It returns inference speed as latency (milliseconds per image) and throughput (frames per second) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "114ab716-fb96-4cec-bd18-24cd0da65050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark FP32 model (IR)\n",
      "Count:      520 iterations\n",
      "Duration:   60833.47 ms\n",
      "Latency:    465.50 ms\n",
      "Throughput: 8.55 FPS\n",
      "\n",
      "Benchmark INT8 model (IR)\n",
      "Count:      776 iterations\n",
      "Duration:   60435.36 ms\n",
      "Latency:    301.43 ms\n",
      "Throughput: 12.84 FPS\n"
     ]
    }
   ],
   "source": [
    "def parse_benchmark_output(benchmark_output: str):\n",
    "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
    "    parsed_output = [line for line in benchmark_output if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model (IR)')\n",
    "benchmark_output = !benchmark_app -m \"./output/fp32/model_unet_fp32.xml\" -d CPU -api async -t 60\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('\\nBenchmark INT8 model (IR)')\n",
    "benchmark_output = !benchmark_app -m \"./output/int8//model_unet_int8.xml\" -d CPU -api async -t 60\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e73cd89-1a3d-4329-8074-720d6d0a6e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'           Intel(R) Xeon(R) CPU @ 2.30GHz'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie = IECore()\n",
    "ie.get_metric(device_name=\"CPU\", metric_name=\"FULL_DEVICE_NAME\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
